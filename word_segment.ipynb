{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from requests import request, ConnectionError\n",
    "from json import loads\n",
    "\n",
    "word_dict = defaultdict(None)\n",
    "label_func = defaultdict(None)\n",
    "known_prefixes = set()\n",
    "known_suffixes = set()\n",
    "\n",
    "EVQR_AFFIX = '<evqr.affix>'\n",
    "PREFIX_AND_SUFFIX = '<prefix.and.suffix>'\n",
    "VOWEL = '<vowel>'\n",
    "\n",
    "try:\n",
    "    with open('.env.json') as f:\n",
    "        ENV_VARIABLES = loads(f.read())\n",
    "        f.close()\n",
    "except FileNotFoundError:\n",
    "    ENV_VARIABLES = {'DATA_DIR': 'C:\\\\'}\n",
    "DATA_DIR = ENV_VARIABLES['DATA_DIR']\n",
    "FTP_DIR = 'http://m106.nthu.edu.tw/~s106062341/morpheme_finder_data/'\n",
    "\n",
    "\n",
    "class Word:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_synonym_postfix(word, delete=None, append=None):\n",
    "        return f'{word}{f\"--{delete}--\" if delete is not None else \"\"}{f\"++{append}++\" if append is not None else \"\"}'\n",
    "\n",
    "    @staticmethod\n",
    "    def create_synonym_prefix(word, delete=None, append=None):\n",
    "        return f'{f\"--{delete}--\" if delete is not None else \"\"}{f\"++{append}++\" if append is not None else \"\"}{word}'\n",
    "\n",
    "    @staticmethod\n",
    "    def letter_cmp(a, b):\n",
    "        divider = 0\n",
    "        for i, (letter_a, letter_b) in enumerate(zip(a, b)):\n",
    "            if letter_a != letter_b:\n",
    "                divider = i\n",
    "        return min(divider, len(a), len(b))\n",
    "\n",
    "    def __init__(self, text, affix_list):\n",
    "        self.text = text\n",
    "        self.affix_list = affix_list\n",
    "        self.synonym = defaultdict(None)\n",
    "        self.label = defaultdict(None)\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        return sum([c for c in self.synonym.values()])\n",
    "\n",
    "    def create_label(self, label_name, *args):\n",
    "        if label_name not in label_func:\n",
    "            return False\n",
    "        self.label[label_name] = label_func[label_name](self, *args)\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_file(filename: str, callback: classmethod) -> bool:\n",
    "    try:\n",
    "        with open(f'{DATA_DIR}{filename}', 'r') as f:\n",
    "            callback(f.read())\n",
    "            f.close()\n",
    "            return True\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            res = request('GET', f'{FTP_DIR}{filename}')\n",
    "            res.encoding = 'Big5'\n",
    "            callback(res.text)\n",
    "            return True\n",
    "        except ConnectionError:\n",
    "            print('HTTP connection failed')\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f'Load failed: {e}')\n",
    "            return False\n",
    "\n",
    "\n",
    "def load_vocabulary():\n",
    "    def callback(content):\n",
    "        for line in content.split('\\n')[1:-1]:\n",
    "            word, *affix_list = line.replace('-', '').split(' ')[:-1]\n",
    "            word_dict[word] = (Word(word, affix_list))\n",
    "    if get_file('EVQR.word.and.affix.txt', callback):\n",
    "        print('Load done')\n",
    "\n",
    "\n",
    "def load_prefix_and_suffix():\n",
    "\n",
    "    def prefix_callback(content):\n",
    "        for line in content.split('\\n')[1:-1]:\n",
    "            known_prefixes.update(filter(lambda x: len(x) > 0, line[:-1].strip().replace('-', '').split(', ')))\n",
    "\n",
    "    def suffix_callback(content):\n",
    "        for line in content.split('\\n'):\n",
    "            known_suffixes.update(filter(lambda x: len(x) > 0, line[:-1].strip().replace('-', '').split(', ')))\n",
    "\n",
    "    if get_file('prefixes.txt', prefix_callback) and get_file('suffixes.txt', suffix_callback):\n",
    "        print('Load prefixes & suffixes done')\n",
    "\n",
    "\n",
    "def mapping_label_func():\n",
    "    def evqr_affix(word):\n",
    "        text = word.text\n",
    "        label = [0] * len(text)\n",
    "        pos = 0\n",
    "        for affix in word.affix_list:\n",
    "            if affix.lower() in text:\n",
    "                label[text.find(affix, pos)] = 1 if pos != 0 else 0\n",
    "                pos = text.find(affix, pos) + len(affix)\n",
    "            else:\n",
    "                k = Word.letter_cmp(text[pos:], affix)\n",
    "                if k > 1:\n",
    "                    label[pos] = 1 if pos != 0 else 0\n",
    "                    pos += 1\n",
    "\n",
    "        return [t for t in zip(text, label)]\n",
    "\n",
    "    def vowel(word):\n",
    "        vowels = {\"a\", \"e\", \"i\", \"o\", \"u\"}\n",
    "        return [(letter, int(letter in vowels)) for letter in word.text]\n",
    "\n",
    "    def prefix_and_suffix(word):\n",
    "        word_len = len(word.text)\n",
    "        label = [0] * word_len\n",
    "\n",
    "        for i in range(word_len):\n",
    "            pattern = word.text[:word_len - 1 - i]\n",
    "            if pattern in known_prefixes:\n",
    "                label[len(pattern)] = 1\n",
    "\n",
    "        for i in range(word_len):\n",
    "            pattern = word.text[i + 1:]\n",
    "            if pattern in known_suffixes:\n",
    "                label[i] = 2 if label[i] == 0 else 3\n",
    "\n",
    "        return [t for t in zip(word.text, label)]\n",
    "\n",
    "    label_func[EVQR_AFFIX] = evqr_affix\n",
    "    label_func[VOWEL] = vowel\n",
    "    label_func[PREFIX_AND_SUFFIX] = prefix_and_suffix\n",
    "    print('Mapping done')\n",
    "\n",
    "\n",
    "def create_label_data():\n",
    "    for word in tqdm(word_dict.values()):\n",
    "        if not word.create_label(EVQR_AFFIX):\n",
    "            print('Failed at label with EVQR.affix')\n",
    "            return False\n",
    "        if not word.create_label(VOWEL):\n",
    "            print('Failed at label with Vowel')\n",
    "            return False\n",
    "        if not word.create_label(PREFIX_AND_SUFFIX):\n",
    "            print('Failed at label with prefix & suffix')\n",
    "            return False\n",
    "    print('Label done')\n",
    "    return True\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     load_vocabulary()\n",
    "#     load_prefix_and_suffix()\n",
    "#     mapping_label_func()\n",
    "#     if create_label_data():\n",
    "#         for w in word_dict.values():\n",
    "#             print(w.label[EVQR_AFFIX])  # data base on EVQR.word.and.suffix.txt\n",
    "#             # print(w.label[PREFIX_AND_SUFFIX])  # data base on prefixes.txt & suffixes.txt\n",
    "#             # print(w.label[VOWEL])  # data base on vowel's position in the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5237/5237 [00:00<00:00, 57683.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load done\n",
      "Load prefixes & suffixes done\n",
      "Mapping done\n",
      "Label done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_words = []\n",
    "\n",
    "\n",
    "\n",
    "load_vocabulary()\n",
    "load_prefix_and_suffix()\n",
    "mapping_label_func()\n",
    "if create_label_data():\n",
    "    for w in word_dict.values():\n",
    "#         print(w.label[EVQR_AFFIX])  # data base on EVQR.word.and.suffix.txt\n",
    "        # print(w.label[PREFIX_AND_SUFFIX])  # data base on prefixes.txt & suffixes.txt\n",
    "#         print(w.label[VOWEL])  # data base on vowel's position in the word\n",
    "        prepared_words.append(w.label[VOWEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5237"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepared_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_features(word, i):\n",
    "    features = [\n",
    "        'bias',\n",
    "        'char=' + word[i][0] \n",
    "    ]\n",
    "\n",
    "    if i >= 1:\n",
    "        features.extend([\n",
    "            'char-1=' + word[i-1][0],\n",
    "            'char-1:0=' + word[i-1][0] + word[i][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "\n",
    "    if i >= 2:\n",
    "        features.extend([\n",
    "            'char-2=' + word[i-2][0],\n",
    "            'char-2:0=' + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "            'char-2:-1=' + word[i-2][0] + word[i-1][0],\n",
    "        ])\n",
    "\n",
    "#     if i >= 3:\n",
    "#         features.extend([\n",
    "#             'char-3:0=' + word[i-3][0] + word[i-2][0] + word[i-1][0] + word[i][0],\n",
    "#             'char-3:-1=' + word[i-3][0] + word[i-2][0] + word[i-1][0],\n",
    "#         ])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_features(prepared_word):\n",
    "    return [create_char_features(prepared_word, i) for i in range(len(prepared_word))]\n",
    "\n",
    "def create_word_labels(prepared_word):\n",
    "    return [str(part[1]) for part in prepared_word]\n",
    "\n",
    "X = [create_word_features(pw) for pw in prepared_words[:4000]]\n",
    "y = [create_word_labels(pw)   for pw in prepared_words[:4000]]\n",
    "\n",
    "X_test = [create_word_features(pw) for pw in prepared_words[4000:]]\n",
    "y_test = [create_word_labels(pw)   for pw in prepared_words[4000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bias', 'char=p', 'BOS'],\n",
       " ['bias', 'char=h', 'char-1=p', 'char-1:0=ph'],\n",
       " ['bias',\n",
       "  'char=y',\n",
       "  'char-1=h',\n",
       "  'char-1:0=hy',\n",
       "  'char-2=p',\n",
       "  'char-2:0=phy',\n",
       "  'char-2:-1=ph'],\n",
       " ['bias',\n",
       "  'char=l',\n",
       "  'char-1=y',\n",
       "  'char-1:0=yl',\n",
       "  'char-2=h',\n",
       "  'char-2:0=hyl',\n",
       "  'char-2:-1=hy'],\n",
       " ['bias',\n",
       "  'char=l',\n",
       "  'char-1=l',\n",
       "  'char-1:0=ll',\n",
       "  'char-2=y',\n",
       "  'char-2:0=yll',\n",
       "  'char-2:-1=yl'],\n",
       " ['bias',\n",
       "  'char=o',\n",
       "  'char-1=l',\n",
       "  'char-1:0=lo',\n",
       "  'char-2=l',\n",
       "  'char-2:0=llo',\n",
       "  'char-2:-1=ll'],\n",
       " ['bias',\n",
       "  'char=d',\n",
       "  'char-1=o',\n",
       "  'char-1:0=od',\n",
       "  'char-2=l',\n",
       "  'char-2:0=lod',\n",
       "  'char-2:-1=lo'],\n",
       " ['bias',\n",
       "  'char=e',\n",
       "  'char-1=d',\n",
       "  'char-1:0=de',\n",
       "  'char-2=o',\n",
       "  'char-2:0=ode',\n",
       "  'char-2:-1=od']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_word_features(\"phyllode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X, y):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,\n",
    "    'c2': 1e-3,\n",
    "    'max_iterations': 50,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train('word-segmentation.crfsuite')\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('word-segmentation.crfsuite')\n",
    "\n",
    "\n",
    "def segment_word(word):\n",
    "    w = word.replace(\" \", \"\")\n",
    "    prediction = tagger.tag(create_word_features(w))\n",
    "    complete = \"\"\n",
    "    for i, p in enumerate(prediction):\n",
    "        if int(p) >= 1:\n",
    "            complete += \" \" + w[i]\n",
    "        else:\n",
    "            complete += w[i]\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phyll od e'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_word(\"phyllode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}